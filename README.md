# ğŸ  Home Sales Analysis with PySpark

![PySpark](https://img.shields.io/badge/PySpark-Big%20Data-orange?logo=apachespark)
![Python](https://img.shields.io/badge/Python-Data%20Processing-blue?logo=python)
![Google Colab](https://img.shields.io/badge/Google%20Colab-Notebook-blue?logo=googlecolab&logoColor=white)
![AWS](https://img.shields.io/badge/AWS-S3%20Data-green?logo=amazonaws)

## ğŸ“Œ Project Overview
This project leverages **PySpark** to analyze home sales data efficiently at scale. The dataset is sourced from an **AWS S3 bucket**, loaded into a **Spark DataFrame**, and queried using **SparkSQL**.

### ğŸš€ **Key Tasks Completed**
âœ… Load & preprocess real estate data  
âœ… Compute **average home prices** based on multiple conditions  
âœ… Improve performance with **caching & partitioning**  
âœ… Utilize **Parquet format** for optimized storage and retrieval  
âœ… Compare **runtime improvements** with different query approaches  

---

## ğŸ“‚ **Dataset Information**
- **Source**: [AWS S3 - Home Sales Data](https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-classroom/v1.2/22-big-data/home_sales_revised.csv)
- **Columns:**
  - **Property Details:** `bedrooms`, `bathrooms`, `floors`, `sqft_living`, `view`, etc.
  - **Sale Information:** `date_sold`, `price`, `waterfront`, etc.
  - **Build Information:** `date_built`

---

## ğŸ”§ **Setup & Installation**
### 1ï¸âƒ£ Install Dependencies
```bash
pip install pyspark findspark
```

### 2ï¸âƒ£ Run the Script
Ensure to run the file on Google Colab:
```bash
Home_Sales_main_code_colab.ipynb
```

---

## ğŸ” **Key Queries & Results**
### ğŸ  **1. Average Price of Four-Bedroom Homes Per Year**
```sql
SELECT YEAR(date) AS year_sold, ROUND(AVG(price), 2) AS avg_price
FROM home_sales
WHERE bedrooms = 4
GROUP BY year_sold
ORDER BY year_sold;
```
| Year Sold | Avg Price ($) |
|-----------|--------------|
| 2019      | 300,263.70   |
| 2020      | 298,353.78   |
| 2021      | 301,819.44   |
| 2022      | 296,363.88   |

---

### ğŸ›  **2. Cached vs. Uncached Query Performance**
| Query Type | **Uncached Runtime** | **Cached Runtime** |
|------------|------------------|----------------|
| Avg Price by View (â‰¥ $350K) | â³ 0.95 sec | âš¡ 0.47 sec |
| Partitioned Data Query | â³ 0.93 sec | âš¡ Optimized |

Caching significantly reduces processing time, improving **query speed by ~50%**.

---

## ğŸ“Š **Performance Optimization Techniques**
âœ… **Caching tables** improves query efficiency  
âœ… **Partitioning by `date_built`** enhances retrieval speeds  
âœ… **Using Parquet format** reduces storage overhead  

---

## ğŸ¯ **Next Steps**
ğŸ”¹ Apply **ML models** to predict housing prices  
ğŸ”¹ Automate **data ingestion** from AWS  
ğŸ”¹ Enhance **visualization using Matplotlib or Seaborn**  

---

## ğŸ¤ **Contributing**
Pull requests are welcome!  
Feel free to **fork the repository**, create a branch, and submit changes.

---

## ğŸ“„ **License**
MIT License. Free to use and modify.

---

## ğŸ‘¨â€ğŸ’» **Author**
**Your Name**  
ğŸ”— [LinkedIn](https://linkedin.com/in/yourprofile) | [GitHub](https://github.com/yourprofile)
```

---

### âœ… **What This README Provides**
ğŸ“Œ **Project Overview**  
ğŸ“‚ **Dataset Details**  
ğŸš€ **Installation & Setup Guide**  
ğŸ” **Key Queries & Results**  
ğŸ“Š **Performance Optimization**  
ğŸ¤ **Contribution & Licensing Info**  

This README will make your GitHub project **look professional & well-documented**! Let me know if you need any tweaks. ğŸš€âœ¨

